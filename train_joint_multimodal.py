#!/usr/bin/env python3
"""
è”åˆå¤šæ¨¡æ€åˆ†å­ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬
å®ç°çœŸæ­£çš„å¤šæ¨¡æ€è”åˆè®­ç»ƒï¼ŒåŒ…æ‹¬æ¨¡æ€ç‰¹å¾å¯¹é½
"""

import os
import sys
import argparse
import logging
import json
import time
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from transformers import T5ForConditionalGeneration, T5Tokenizer
from scaffold_mol_gen.models.end2end_model import End2EndMolecularGenerator
from scaffold_mol_gen.training.metrics import GenerationMetrics
from scaffold_mol_gen.utils.mol_utils import MolecularUtils
from scaffold_mol_gen.data.multimodal_preprocessor import MultiModalPreprocessor
from rdkit import Chem

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class JointMultiModalDataset(Dataset):
    """è”åˆå¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ”¯æŒåŠ¨æ€æ¨¡æ€åˆ‡æ¢"""
    
    def __init__(self, csv_file: str, 
                 molt5_tokenizer: T5Tokenizer,
                 modality_mix: Dict[str, float] = None,
                 max_text_length: int = 128,
                 max_smiles_length: int = 128,
                 filter_invalid: bool = True):
        """
        Args:
            csv_file: CSVæ•°æ®æ–‡ä»¶è·¯å¾„
            molt5_tokenizer: MolT5 tokenizer
            modality_mix: æ¨¡æ€æ··åˆæ¯”ä¾‹ {'smiles': 0.4, 'graph': 0.3, 'image': 0.3}
            max_text_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
            max_smiles_length: æœ€å¤§SMILESé•¿åº¦
            filter_invalid: æ˜¯å¦è¿‡æ»¤æ— æ•ˆæ•°æ®
        """
        self.tokenizer = molt5_tokenizer
        self.max_text_length = max_text_length
        self.max_smiles_length = max_smiles_length
        self.preprocessor = MultiModalPreprocessor()
        
        # é»˜è®¤æ¨¡æ€æ··åˆæ¯”ä¾‹
        if modality_mix is None:
            modality_mix = {'smiles': 0.4, 'graph': 0.3, 'image': 0.3}
        self.modality_mix = modality_mix
        self.modalities = list(modality_mix.keys())
        
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        logger.info(f"åŠ è½½è”åˆè®­ç»ƒæ•°æ®é›†: {csv_file}")
        df = pd.read_csv(csv_file)
        
        # æ•°æ®æ¸…æ´—å’ŒéªŒè¯
        self.data = []
        valid_count = 0
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†æ•°æ®"):
            scaffold = str(row.get('scaffold', row.get('SMILES', ''))).strip()
            text = str(row.get('description', row.get('text', ''))).strip()
            target_smiles = str(row.get('SMILES', row.get('target', ''))).strip()
            
            # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
            if filter_invalid:
                # éªŒè¯ç›®æ ‡SMILES
                if not target_smiles or target_smiles in ['nan', 'None']:
                    continue
                
                mol = Chem.MolFromSmiles(target_smiles)
                if mol is None:
                    continue
                
                # è§„èŒƒåŒ–SMILES
                target_smiles = Chem.MolToSmiles(mol, canonical=True)
                
                # éªŒè¯scaffold
                if not scaffold or scaffold in ['nan', 'None']:
                    scaffold = ""  # ç©ºscaffold
                elif Chem.MolFromSmiles(scaffold) is None:
                    continue  # æ— æ•ˆscaffold
                
                # éªŒè¯æ–‡æœ¬æè¿°
                if not text or text in ['nan', 'None']:
                    text = ""  # ç©ºæ–‡æœ¬æè¿°
            
            # æˆªæ–­é•¿åº¦
            text = text[:max_text_length*2]  # é¢„ç•™tokenizationç©ºé—´
            target_smiles = target_smiles[:max_smiles_length*2]
            
            self.data.append({
                'scaffold': scaffold,
                'text': text,
                'target_smiles': target_smiles
            })
            valid_count += 1
        
        logger.info(f"è”åˆæ•°æ®é›†åŠ è½½å®Œæˆ: {len(df)} -> {valid_count} æœ‰æ•ˆæ ·æœ¬")
        logger.info(f"æ¨¡æ€æ··åˆæ¯”ä¾‹: {modality_mix}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # éšæœºé€‰æ‹©æ¨¡æ€
        modality = np.random.choice(
            self.modalities, 
            p=list(self.modality_mix.values())
        )
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬ï¼ˆMolT5æ ¼å¼ï¼‰
        scaffold_text = f"Scaffold: {sample['scaffold']}" if sample['scaffold'] else "Scaffold: <empty>"
        description_text = f"Description: {sample['text']}" if sample['text'] else "Description: <empty>"
        input_text = f"{scaffold_text} {description_text}"
        
        # Tokenizeè¾“å…¥
        input_encoding = self.tokenizer(
            input_text,
            max_length=self.max_text_length,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        
        # Tokenizeç›®æ ‡
        target_encoding = self.tokenizer(
            sample['target_smiles'],
            max_length=self.max_smiles_length,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        
        return {
            'input_ids': input_encoding['input_ids'].squeeze(0),
            'attention_mask': input_encoding['attention_mask'].squeeze(0),
            'labels': target_encoding['input_ids'].squeeze(0),
            'target_attention_mask': target_encoding['attention_mask'].squeeze(0),
            'scaffold_data': sample['scaffold'],
            'text_data': sample['text'],
            'target_smiles': sample['target_smiles'],
            'scaffold_modality': modality  # åŠ¨æ€æ¨¡æ€
        }


class MultiModalAlignmentLoss(nn.Module):
    """å¤šæ¨¡æ€ç‰¹å¾å¯¹é½æŸå¤±"""
    
    def __init__(self, hidden_size: int = 768, temperature: float = 0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.temperature = temperature
        
        # æ¨¡æ€æŠ•å½±å±‚
        self.smiles_proj = nn.Linear(hidden_size, hidden_size)
        self.graph_proj = nn.Linear(hidden_size, hidden_size)
        self.image_proj = nn.Linear(hidden_size, hidden_size)
        self.text_proj = nn.Linear(hidden_size, hidden_size)
        
    def forward(self, modality_features: Dict[str, torch.Tensor], 
                text_features: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ¨¡æ€å¯¹é½æŸå¤±
        Args:
            modality_features: {'smiles': tensor, 'graph': tensor, 'image': tensor}
            text_features: æ–‡æœ¬ç‰¹å¾
        Returns:
            å¯¹é½æŸå¤±
        """
        total_loss = 0.0
        num_pairs = 0
        
        # æŠ•å½±æ–‡æœ¬ç‰¹å¾
        text_proj = self.text_proj(text_features)  # [batch_size, hidden_size]
        text_proj = F.normalize(text_proj, p=2, dim=-1)
        
        # è®¡ç®—æ¯ä¸ªæ¨¡æ€ä¸æ–‡æœ¬çš„å¯¹é½æŸå¤±
        for modality, features in modality_features.items():
            if features is None:
                continue
                
            # é€‰æ‹©å¯¹åº”çš„æŠ•å½±å±‚
            if modality == 'smiles':
                modal_proj = self.smiles_proj(features)
            elif modality == 'graph':
                modal_proj = self.graph_proj(features)
            elif modality == 'image':
                modal_proj = self.image_proj(features)
            else:
                continue
            
            modal_proj = F.normalize(modal_proj, p=2, dim=-1)
            
            # è®¡ç®—å¯¹æ¯”æŸå¤±
            similarity = torch.matmul(modal_proj, text_proj.T) / self.temperature
            
            # æ­£æ ·æœ¬åœ¨å¯¹è§’çº¿ä¸Š
            batch_size = similarity.size(0)
            labels = torch.arange(batch_size, device=similarity.device)
            
            # å¯¹ç§°å¯¹æ¯”æŸå¤±
            loss_modal_to_text = F.cross_entropy(similarity, labels)
            loss_text_to_modal = F.cross_entropy(similarity.T, labels)
            
            total_loss += (loss_modal_to_text + loss_text_to_modal) / 2
            num_pairs += 1
        
        return total_loss / num_pairs if num_pairs > 0 else torch.tensor(0.0)


class JointMultiModalTrainer:
    """è”åˆå¤šæ¨¡æ€è®­ç»ƒå™¨"""
    
    def __init__(self,
                 model: End2EndMolecularGenerator,
                 train_dataset: JointMultiModalDataset,
                 val_dataset: JointMultiModalDataset,
                 config: Dict[str, Any],
                 device: str = 'cuda'):
        """
        Args:
            model: ç«¯åˆ°ç«¯æ¨¡å‹
            train_dataset: è®­ç»ƒæ•°æ®é›†
            val_dataset: éªŒè¯æ•°æ®é›†
            config: è®­ç»ƒé…ç½®
            device: è®¾å¤‡
        """
        self.model = model.to(device)
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.config = config
        self.device = device
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=config.get('batch_size', 8),
            shuffle=True,
            num_workers=config.get('num_workers', 4),
            drop_last=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=config.get('batch_size', 8),
            shuffle=False,
            num_workers=config.get('num_workers', 4),
            drop_last=False
        )
        
        # è®­ç»ƒå‚æ•°
        self.num_epochs = config.get('num_epochs', 5)
        self.learning_rate = config.get('learning_rate', 1e-4)
        self.weight_decay = config.get('weight_decay', 1e-5)
        self.max_grad_norm = config.get('max_grad_norm', 1.0)
        self.save_interval = config.get('save_interval', 1000)
        self.eval_interval = config.get('eval_interval', 500)
        self.log_interval = config.get('log_interval', 100)
        self.alignment_weight = config.get('alignment_weight', 0.1)
        
        # Tokenizerçº¦æŸ
        self.tokenizer = train_dataset.tokenizer
        self.vocab_size = self.tokenizer.vocab_size
        
        # å¯¹é½æŸå¤±
        self.alignment_loss = MultiModalAlignmentLoss(
            hidden_size=config.get('hidden_size', 768),
            temperature=config.get('alignment_temperature', 0.1)
        ).to(device)
        
        # ä¼˜åŒ–å™¨ï¼ˆåŒ…å«å¯¹é½æŸå¤±çš„å‚æ•°ï¼‰
        all_params = list(self.model.parameters()) + list(self.alignment_loss.parameters())
        trainable_params = [p for p in all_params if p.requires_grad]
        
        self.optimizer = torch.optim.AdamW(
            trainable_params,
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=self.learning_rate,
            epochs=self.num_epochs,
            steps_per_epoch=len(self.train_loader),
            pct_start=0.1
        )
        
        # è¯„ä»·æŒ‡æ ‡
        self.metrics = GenerationMetrics()
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.epoch = 0
        self.best_val_loss = float('inf')
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path(config.get('output_dir', 'outputs/joint_training'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(self.output_dir / 'tensorboard')
        
        # ä¿å­˜é…ç½®
        with open(self.output_dir / 'config.json', 'w') as f:
            json.dump(config, f, indent=2)
        
        logger.info(f"è”åˆè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - è®¾å¤‡: {device}")
        logger.info(f"  - è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        logger.info(f"  - éªŒè¯æ ·æœ¬: {len(val_dataset)}")
        logger.info(f"  - æ‰¹å¤§å°: {config.get('batch_size', 8)}")
        logger.info(f"  - å­¦ä¹ ç‡: {self.learning_rate}")
        logger.info(f"  - å¯¹é½æƒé‡: {self.alignment_weight}")
        logger.info(f"  - å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainable_params):,}")
    
    def compute_joint_loss(self, batch: Dict[str, Any]) -> Tuple[torch.Tensor, Dict[str, float]]:
        """è®¡ç®—è”åˆæŸå¤±ï¼ˆç”ŸæˆæŸå¤± + å¯¹é½æŸå¤±ï¼‰"""
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        input_ids = batch['input_ids'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        labels = batch['labels'].to(self.device)
        scaffold_modality = batch['scaffold_modality']
        
        # 1. åŸºç¡€ç”ŸæˆæŸå¤±
        outputs = self.model.molt5_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        generation_loss = outputs.loss
        
        # çº¦æŸlogitsåˆ°æœ‰æ•ˆè¯æ±‡è¡¨èŒƒå›´
        if hasattr(outputs, 'logits'):
            logits = outputs.logits
            if logits.size(-1) > self.vocab_size:
                invalid_mask = torch.zeros_like(logits)
                invalid_mask[:, :, self.vocab_size:] = -float('inf')
                logits = logits + invalid_mask
                
                # é‡æ–°è®¡ç®—æŸå¤±
                loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                if labels is not None:
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = labels[..., 1:].contiguous()
                    generation_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), 
                                             shift_labels.view(-1))
        
        # 2. å¤šæ¨¡æ€å¯¹é½æŸå¤±
        alignment_loss = torch.tensor(0.0, device=self.device)
        
        try:
            # è·å–å¤šæ¨¡æ€ç‰¹å¾
            scaffold_data = [batch['scaffold_data'][i] for i in range(len(batch['scaffold_data']))]
            text_data = [batch['text_data'][i] for i in range(len(batch['text_data']))]
            
            # ç¼–ç ä¸åŒæ¨¡æ€çš„ç‰¹å¾
            modality_features = {}
            text_features = None
            
            # æŒ‰æ‰¹æ¬¡ä¸­çš„æ¨¡æ€åˆ†ç»„
            modality_groups = {}
            for i, modality in enumerate(scaffold_modality):
                if modality not in modality_groups:
                    modality_groups[modality] = []
                modality_groups[modality].append(i)
            
            # ä¸ºæ¯ç§æ¨¡æ€ç¼–ç ç‰¹å¾
            for modality, indices in modality_groups.items():
                batch_scaffold = [scaffold_data[i] for i in indices]
                batch_text = [text_data[i] for i in indices]
                
                # ç¼–ç scaffoldç‰¹å¾
                if modality == 'smiles':
                    scaffold_features = self.model.encoder.smiles_encoder(batch_scaffold)
                elif modality == 'graph':
                    # è¿™é‡Œéœ€è¦é¢„å¤„ç†ä¸ºå›¾æ•°æ®
                    graph_data = [self.model.encoder.graph_encoder.preprocessor.smiles_to_graph(s) for s in batch_scaffold]
                    scaffold_features = self.model.encoder.graph_encoder(graph_data)
                elif modality == 'image':
                    # è¿™é‡Œéœ€è¦é¢„å¤„ç†ä¸ºå›¾åƒæ•°æ®
                    image_data = [self.model.encoder.image_encoder.preprocessor.smiles_to_image(s) for s in batch_scaffold]
                    scaffold_features = self.model.encoder.image_encoder(image_data)
                else:
                    continue
                
                # ç¼–ç æ–‡æœ¬ç‰¹å¾
                batch_text_features = self.model.encoder.text_encoder(batch_text)
                
                modality_features[modality] = scaffold_features
                if text_features is None:
                    text_features = batch_text_features
                else:
                    text_features = torch.cat([text_features, batch_text_features], dim=0)
            
            # è®¡ç®—å¯¹é½æŸå¤±
            if modality_features and text_features is not None:
                alignment_loss = self.alignment_loss(modality_features, text_features)
        
        except Exception as e:
            logger.warning(f"å¯¹é½æŸå¤±è®¡ç®—å¤±è´¥: {e}")
            alignment_loss = torch.tensor(0.0, device=self.device)
        
        # æ€»æŸå¤±
        total_loss = generation_loss + self.alignment_weight * alignment_loss
        
        loss_dict = {
            'total_loss': total_loss.item(),
            'generation_loss': generation_loss.item(),
            'alignment_loss': alignment_loss.item() if isinstance(alignment_loss, torch.Tensor) else 0.0
        }
        
        return total_loss, loss_dict
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        self.alignment_loss.train()
        
        total_losses = {
            'total_loss': 0.0,
            'generation_loss': 0.0,
            'alignment_loss': 0.0
        }
        num_batches = 0
        
        progress_bar = tqdm(
            self.train_loader,
            desc=f"Epoch {self.epoch + 1}/{self.num_epochs}",
            leave=False
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # è®¡ç®—è”åˆæŸå¤±
            total_loss, loss_dict = self.compute_joint_loss(batch)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(
                    [p for p in self.optimizer.param_groups[0]['params'] if p.requires_grad],
                    self.max_grad_norm
                )
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            self.scheduler.step()
            
            # ç»Ÿè®¡
            for key, value in loss_dict.items():
                total_losses[key] += value
            num_batches += 1
            self.global_step += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss_dict["total_loss"]:.4f}',
                'gen': f'{loss_dict["generation_loss"]:.4f}',
                'align': f'{loss_dict["alignment_loss"]:.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })
            
            # æ—¥å¿—è®°å½•
            if self.global_step % self.log_interval == 0:
                for key, value in loss_dict.items():
                    self.writer.add_scalar(f'train/{key}', value, self.global_step)
                self.writer.add_scalar('train/learning_rate', 
                                     self.optimizer.param_groups[0]['lr'], 
                                     self.global_step)
            
            # éªŒè¯
            if self.global_step % self.eval_interval == 0:
                val_metrics = self.validate()
                self.model.train()
                self.alignment_loss.train()
                
                # è®°å½•éªŒè¯æŒ‡æ ‡
                for key, value in val_metrics.items():
                    self.writer.add_scalar(f'val/{key}', value, self.global_step)
                
                logger.info(f"Step {self.global_step} - Val Loss: {val_metrics.get('total_loss', 0):.4f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.global_step % self.save_interval == 0:
                self.save_checkpoint(f'checkpoint_step_{self.global_step}.pt')
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {}
        for key, value in total_losses.items():
            avg_losses[key] = value / num_batches if num_batches > 0 else 0.0
        
        return avg_losses
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        self.alignment_loss.eval()
        
        total_losses = {
            'total_loss': 0.0,
            'generation_loss': 0.0,
            'alignment_loss': 0.0
        }
        num_batches = 0
        generated_smiles = []
        target_smiles_list = []
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validating", leave=False):
                # è®¡ç®—æŸå¤±
                total_loss, loss_dict = self.compute_joint_loss(batch)
                
                for key, value in loss_dict.items():
                    total_losses[key] += value
                num_batches += 1
                
                # ç”Ÿæˆæ ·æœ¬ï¼ˆå°‘é‡ç”¨äºè´¨é‡è¯„ä¼°ï¼‰
                if len(generated_smiles) < 30:  # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡
                    try:
                        # ä½¿ç”¨çº¦æŸç”Ÿæˆ
                        input_ids = batch['input_ids'][:2].to(self.device)
                        attention_mask = batch['attention_mask'][:2].to(self.device)
                        
                        generated_ids = self.model.molt5_model.generate(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            max_length=64,
                            num_beams=3,
                            temperature=0.9,
                            do_sample=True,
                            pad_token_id=self.tokenizer.pad_token_id,
                            eos_token_id=self.tokenizer.eos_token_id,
                            forced_eos_token_id=self.tokenizer.eos_token_id
                        )
                        
                        # çº¦æŸç”Ÿæˆçš„token IDåˆ°æœ‰æ•ˆèŒƒå›´
                        generated_ids = torch.clamp(generated_ids, 0, self.vocab_size - 1)
                        
                        batch_generated = self.tokenizer.batch_decode(
                            generated_ids,
                            skip_special_tokens=True
                        )
                        
                        batch_targets = batch['target_smiles'][:2]
                        
                        generated_smiles.extend(batch_generated)
                        target_smiles_list.extend(batch_targets)
                        
                    except Exception as e:
                        logger.warning(f"éªŒè¯ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        
        # è®¡ç®—å¹³å‡éªŒè¯æŒ‡æ ‡
        val_metrics = {}
        for key, value in total_losses.items():
            val_metrics[key] = value / num_batches if num_batches > 0 else 0.0
        
        # è®¡ç®—åˆ†å­è´¨é‡æŒ‡æ ‡
        if generated_smiles and target_smiles_list:
            try:
                # éªŒè¯ç”Ÿæˆçš„SMILES
                valid_generated = []
                valid_targets = []
                for gen, target in zip(generated_smiles, target_smiles_list):
                    if gen and target:
                        mol = Chem.MolFromSmiles(gen)
                        if mol is not None:
                            valid_generated.append(gen)
                            valid_targets.append(target)
                
                if valid_generated:
                    quality_metrics = self.metrics.compute_metrics(
                        generated_smiles=valid_generated,
                        reference_smiles=valid_targets
                    )
                    val_metrics.update(quality_metrics)
                    
                    logger.info("è”åˆè®­ç»ƒéªŒè¯æŒ‡æ ‡:")
                    logger.info(f"  ç”Ÿæˆæ ·æœ¬: {len(generated_smiles)}, æœ‰æ•ˆ: {len(valid_generated)}")
                    for key, value in quality_metrics.items():
                        if isinstance(value, (int, float)) and key != 'total_loss':
                            logger.info(f"  {key}: {value:.4f}")
                
            except Exception as e:
                logger.warning(f"è´¨é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        
        return val_metrics
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("å¼€å§‹è”åˆå¤šæ¨¡æ€è®­ç»ƒ...")
        logger.info(f"è®­ç»ƒ {self.num_epochs} epochs")
        
        start_time = time.time()
        
        for epoch in range(self.num_epochs):
            self.epoch = epoch
            logger.info(f"\n=== Epoch {epoch + 1}/{self.num_epochs} ===")
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self.train_epoch()
            
            # è®°å½•epochæŒ‡æ ‡
            for key, value in train_metrics.items():
                self.writer.add_scalar(f'train_epoch/{key}', value, epoch)
            
            # Epochç»“æŸæ—¶éªŒè¯
            val_metrics = self.validate()
            
            # è®°å½•éªŒè¯æŒ‡æ ‡
            for key, value in val_metrics.items():
                self.writer.add_scalar(f'val_epoch/{key}', value, epoch)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            val_loss = val_metrics.get('total_loss', float('inf'))
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint('best_model.pt', is_best=True)
                logger.info(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss:.4f})")
            
            # ä¿å­˜epochæ£€æŸ¥ç‚¹
            self.save_checkpoint(f'epoch_{epoch + 1}.pt')
            
            logger.info(f"Epoch {epoch + 1} - Train Loss: {train_metrics.get('total_loss', 0):.4f}, "
                       f"Val Loss: {val_loss:.4f}")
        
        total_time = time.time() - start_time
        logger.info(f"\nâœ… è”åˆè®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/3600:.2f} å°æ—¶")
        
        # æœ€ç»ˆä¿å­˜
        self.save_checkpoint('final_model.pt')
        self.writer.close()
    
    def save_checkpoint(self, filename: str, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'alignment_loss_state_dict': self.alignment_loss.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config,
            'vocab_size': self.vocab_size
        }
        
        checkpoint_path = self.output_dir / filename
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            best_path = self.output_dir / 'model_best.pt'
            torch.save(checkpoint, best_path)


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description='è”åˆå¤šæ¨¡æ€åˆ†å­ç”Ÿæˆæ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--train-data', type=str, default='Datasets/train.csv',
                       help='è®­ç»ƒæ•°æ®CSVæ–‡ä»¶')
    parser.add_argument('--val-data', type=str, default='Datasets/validation.csv',
                       help='éªŒè¯æ•°æ®CSVæ–‡ä»¶')
    parser.add_argument('--output-dir', type=str, 
                       default=f'outputs/joint_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch-size', type=int, default=4, help='æ‰¹å¤§å°')
    parser.add_argument('--epochs', type=int, default=5, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=5e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--alignment-weight', type=float, default=0.1, help='å¯¹é½æŸå¤±æƒé‡')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡')
    parser.add_argument('--sample-size', type=int, default=1500, help='è®­ç»ƒæ ·æœ¬æ•°é™åˆ¶')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(args.train_data):
        logger.error(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.train_data}")
        return
    
    if not os.path.exists(args.val_data):
        logger.error(f"éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.val_data}")
        return
    
    # åˆ›å»ºé…ç½®
    config = {
        'batch_size': args.batch_size,
        'num_epochs': args.epochs,
        'learning_rate': args.lr,
        'weight_decay': 1e-5,
        'max_grad_norm': 1.0,
        'num_workers': 2,
        'save_interval': 300,
        'eval_interval': 150,
        'log_interval': 30,
        'output_dir': args.output_dir,
        'hidden_size': 768,
        'alignment_weight': args.alignment_weight,
        'alignment_temperature': 0.1,
        'max_text_length': 128,
        'max_smiles_length': 128,
        'filter_invalid': True
    }
    
    logger.info(f"è”åˆè®­ç»ƒé…ç½®: {json.dumps(config, indent=2)}")
    
    # åˆå§‹åŒ–tokenizer
    molt5_path = "/root/autodl-tmp/text2Mol-models/MolT5-Large-Caption2SMILES"
    tokenizer = T5Tokenizer.from_pretrained(molt5_path)
    
    # æ¨¡æ€æ··åˆæ¯”ä¾‹
    modality_mix = {'smiles': 0.4, 'graph': 0.3, 'image': 0.3}
    
    # åˆ›å»ºæ•°æ®é›†
    try:
        logger.info("åˆ›å»ºè”åˆè®­ç»ƒæ•°æ®é›†...")
        train_dataset = JointMultiModalDataset(
            csv_file=args.train_data,
            molt5_tokenizer=tokenizer,
            modality_mix=modality_mix,
            max_text_length=config['max_text_length'],
            max_smiles_length=config['max_smiles_length'],
            filter_invalid=config['filter_invalid']
        )
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if args.sample_size and len(train_dataset.data) > args.sample_size:
            train_dataset.data = train_dataset.data[:args.sample_size]
            logger.info(f"é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°ä¸º: {len(train_dataset.data)}")
        
        logger.info("åˆ›å»ºéªŒè¯æ•°æ®é›†...")
        val_dataset = JointMultiModalDataset(
            csv_file=args.val_data,
            molt5_tokenizer=tokenizer,
            modality_mix=modality_mix,
            max_text_length=config['max_text_length'],
            max_smiles_length=config['max_smiles_length'],
            filter_invalid=config['filter_invalid']
        )
        
        # é™åˆ¶éªŒè¯æ ·æœ¬
        if len(val_dataset.data) > 200:
            val_dataset.data = val_dataset.data[:200]
        
        logger.info(f"è”åˆæ•°æ®é›†åˆ›å»ºå®Œæˆ: train={len(train_dataset)}, val={len(val_dataset)}")
        
    except Exception as e:
        logger.error(f"æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # åˆ›å»ºæ¨¡å‹
    try:
        logger.info("åˆ›å»ºç«¯åˆ°ç«¯æ¨¡å‹...")
        model = End2EndMolecularGenerator(
            hidden_size=768,
            molt5_path=molt5_path,
            use_scibert=False,
            freeze_encoders=False,  # è§£å†»ç¼–ç å™¨è¿›è¡Œè”åˆè®­ç»ƒ
            freeze_molt5=False,     # è§£å†»MolT5è¿›è¡Œå¾®è°ƒ
            fusion_type='both',
            device=args.device
        )
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logger.info(f"æ€»å‚æ•°: {total_params:,}, å¯è®­ç»ƒ: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)")
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    try:
        trainer = JointMultiModalTrainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            config=config,
            device=args.device
        )
        
        # å¼€å§‹è®­ç»ƒ
        logger.info("ğŸš€ å¼€å§‹è”åˆå¤šæ¨¡æ€è®­ç»ƒ...")
        trainer.train()
        logger.info("ğŸ‰ è”åˆè®­ç»ƒæˆåŠŸå®Œæˆï¼")
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        if 'trainer' in locals():
            trainer.save_checkpoint('interrupted_checkpoint.pt')
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()