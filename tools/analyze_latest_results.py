#!/usr/bin/env python3
"""
åˆ†ææœ€æ–°çš„è®­ç»ƒç»“æœ
åˆ¤æ–­æ•ˆæœå’Œæ˜¯å¦éœ€è¦ç»§ç»­è®­ç»ƒå…¶ä»–æ¨¡æ€
"""

import pandas as pd
from pathlib import Path

def analyze_latest_results():
    print("ğŸ“Š åˆ†ææœ€æ–°è®­ç»ƒç»“æœ")
    print("=" * 60)
    
    # æ£€æŸ¥æœ€æ–°å®éªŒç›®å½•
    latest_dir = "experiments/demo_multimodal_20250805_184448"
    
    if not Path(latest_dir).exists():
        print(f"âŒ æœªæ‰¾åˆ°æœ€æ–°å®éªŒç›®å½•: {latest_dir}")
        return
    
    print(f"âœ… æ‰¾åˆ°æœ€æ–°å®éªŒç»“æœ: {latest_dir}")
    
    # åˆ†æå„æ¨¡æ€ç»“æœ
    modalities = ['smiles', 'graph', 'image']
    results = {}
    
    for modality in modalities:
        csv_file = f"{latest_dir}/{modality}/inference_results.csv"
        if Path(csv_file).exists():
            df = pd.read_csv(csv_file)
            
            # è®¡ç®—æŒ‡æ ‡
            total_samples = len(df)
            valid_samples = df['valid'].sum()
            validity = valid_samples / total_samples * 100 if total_samples > 0 else 0
            
            results[modality] = {
                'total': total_samples,
                'valid': valid_samples,
                'validity': validity,
                'data': df
            }
            
            print(f"âœ… {modality.upper()}æ¨¡æ€: {valid_samples}/{total_samples} æœ‰æ•ˆ ({validity:.1f}%)")
        else:
            print(f"âŒ æœªæ‰¾åˆ°{modality}æ¨¡æ€ç»“æœæ–‡ä»¶")
    
    # æ·±åº¦åˆ†æ
    print(f"\nğŸ” æ·±åº¦åˆ†æ:")
    print("-" * 40)
    
    # 1. æ£€æŸ¥æ¨¡æ€å·®å¼‚
    if len(results) >= 2:
        validities = [results[m]['validity'] for m in results.keys()]
        max_validity = max(validities)
        min_validity = min(validities)
        diff = max_validity - min_validity
        
        print(f"æ¨¡æ€é—´æœ‰æ•ˆæ€§å·®å¼‚: {diff:.1f}%")
        
        if diff < 10:
            print("âš ï¸ æ¨¡æ€é—´å·®å¼‚å¾ˆå°ï¼Œå¯èƒ½ä½¿ç”¨äº†ç›¸åŒçš„åº•å±‚æ¨¡å‹")
        else:
            print("âœ… æ¨¡æ€é—´æœ‰æ˜æ˜¾å·®å¼‚ï¼Œè¯´æ˜çœŸæ­£å­¦ä¼šäº†å¤šæ¨¡æ€å¤„ç†")
    
    # 2. æ£€æŸ¥ç”Ÿæˆè´¨é‡
    if 'smiles' in results:
        smiles_data = results['smiles']['data']
        
        # åˆ†æç”Ÿæˆçš„SMILES
        generated_lengths = smiles_data['generated'].str.len()
        avg_length = generated_lengths.mean()
        
        print(f"ç”ŸæˆSMILESå¹³å‡é•¿åº¦: {avg_length:.1f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤ç”Ÿæˆ
        unique_generated = smiles_data['generated'].nunique()
        total_generated = len(smiles_data)
        uniqueness = unique_generated / total_generated * 100 if total_generated > 0 else 0
        
        print(f"ç”Ÿæˆå”¯ä¸€æ€§: {uniqueness:.1f}%")
        
        # æ˜¾ç¤ºå‡ ä¸ªæˆåŠŸç¤ºä¾‹
        valid_samples = smiles_data[smiles_data['valid'] == True]
        if len(valid_samples) > 0:
            print(f"\nâœ… æˆåŠŸç”Ÿæˆç¤ºä¾‹:")
            for i, row in valid_samples.head(3).iterrows():
                target = row['target'][:50] + "..." if len(row['target']) > 50 else row['target']
                generated = row['generated'][:50] + "..." if len(row['generated']) > 50 else row['generated']
                print(f"  æ ·æœ¬{i}: ç›®æ ‡ â†’ ç”Ÿæˆ")
                print(f"    {target}")
                print(f"    {generated}")
    
    # 3. è®­ç»ƒæ•ˆæœè¯„ä¼°
    print(f"\nğŸ¯ è®­ç»ƒæ•ˆæœè¯„ä¼°:")
    print("-" * 40)
    
    if 'smiles' in results:
        smiles_validity = results['smiles']['validity']
        
        if smiles_validity >= 70:
            print("ğŸ‰ ä¼˜ç§€ï¼SMILESæ¨¡æ€è®­ç»ƒæ•ˆæœå¾ˆå¥½")
            verdict = "excellent"
        elif smiles_validity >= 50:
            print("âœ… è‰¯å¥½ï¼SMILESæ¨¡æ€æœ‰åŸºæœ¬çš„ç”Ÿæˆèƒ½åŠ›")  
            verdict = "good"
        elif smiles_validity >= 30:
            print("âš ï¸ ä¸€èˆ¬ã€‚SMILESæ¨¡æ€éœ€è¦æ”¹è¿›")
            verdict = "fair"
        else:
            print("âŒ è¾ƒå·®ã€‚SMILESæ¨¡æ€éœ€è¦é‡æ–°è®­ç»ƒ")
            verdict = "poor"
    
    # 4. å¤šæ¨¡æ€è®­ç»ƒå»ºè®®
    print(f"\nğŸ’¡ å¤šæ¨¡æ€è®­ç»ƒå»ºè®®:")
    print("-" * 40)
    
    if len(results) == 3:  # æœ‰ä¸‰ä¸ªæ¨¡æ€ç»“æœ
        all_validities = [results[m]['validity'] for m in results.keys()]
        
        if max(all_validities) - min(all_validities) < 5:
            print("âš ï¸ å½“å‰ç»“æœå¯èƒ½æ˜¯ç”¨åŒä¸€ä¸ªSMILESæ¨¡å‹æµ‹è¯•çš„")
            print("ğŸ“‹ å»ºè®®ï¼šéœ€è¦åˆ†åˆ«è®­ç»ƒGraphå’ŒImageæ¨¡æ€")
            print("   1. python safe_background_training.py graph")
            print("   2. python safe_background_training.py image")
            need_training = True
        else:
            print("âœ… ä¸‰ä¸ªæ¨¡æ€éƒ½å·²ç‹¬ç«‹è®­ç»ƒï¼Œæœ‰æ˜æ˜¾å·®å¼‚")
            print("ğŸ‰ å¤šæ¨¡æ€è®­ç»ƒå·²å®Œæˆï¼")
            need_training = False
    else:
        print("âŒ ç¼ºå°‘æŸäº›æ¨¡æ€çš„ç»“æœ")
        need_training = True
    
    # 5. åå°è¿è¡Œèƒ½åŠ›
    print(f"\nğŸ”„ å…³äºåå°è¿è¡Œ:")
    print("-" * 40)
    print("âœ… å®‰å…¨è®­ç»ƒè„šæœ¬æ”¯æŒå®Œå…¨åå°è¿è¡Œ")
    print("âœ… åŒ…å«ç£ç›˜ç©ºé—´ç›‘æ§å’Œè‡ªåŠ¨ä¿æŠ¤")
    print("âœ… å¯ä»¥åŒæ—¶è¿è¡Œè®­ç»ƒå’Œç›‘æ§")
    print()
    print("å¯åŠ¨æ–¹å¼:")
    print("  ç»ˆç«¯1: python safe_background_training.py graph")  
    print("  ç»ˆç«¯2: python training_monitor.py")
    
    # æ€»ç»“å»ºè®®
    print(f"\nğŸ† æ€»ç»“å»ºè®®:")
    print("=" * 60)
    
    if 'smiles' in results and results['smiles']['validity'] >= 50:
        print("âœ… SMILESæ¨¡æ€è®­ç»ƒæˆåŠŸï¼Œæœ‰åŸºæœ¬çš„åˆ†å­ç”Ÿæˆèƒ½åŠ›")
        
        if need_training:
            print("ğŸ”„ ä¸‹ä¸€æ­¥ï¼šç»§ç»­è®­ç»ƒGraphå’ŒImageæ¨¡æ€ä»¥è·å¾—å®Œæ•´å¤šæ¨¡æ€èƒ½åŠ›")
            print("âš¡ æ¨èä½¿ç”¨å®‰å…¨è®­ç»ƒè„šæœ¬ï¼Œé¿å…ç£ç›˜ç©ºé—´é—®é¢˜")
            print()
            print("ç«‹å³å¼€å§‹ï¼š")
            print("  python safe_background_training.py graph")
        else:
            print("ğŸ‰ å¤šæ¨¡æ€è®­ç»ƒå·²å®Œæˆï¼å¯ä»¥å¼€å§‹åº”ç”¨")
    else:
        if 'smiles' in results:
            print("âš ï¸ SMILESæ¨¡æ€æ•ˆæœä¸å¤Ÿç†æƒ³ï¼Œå»ºè®®é‡æ–°è®­ç»ƒ")
        else:
            print("âŒ ç¼ºå°‘SMILESæ¨¡æ€ç»“æœ")

if __name__ == "__main__":
    analyze_latest_results()