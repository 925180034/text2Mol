# Default configuration for Scaffold-based Molecular Generation

# Model Configuration
model:
  type: "ScaffoldBasedMolT5Generator"
  hidden_size: 768
  num_attention_heads: 12
  num_fusion_layers: 6
  dropout: 0.1
  
  # MolT5 Configuration
  molt5_checkpoint: "models/MolT5-Small"
  
  # Encoder Configuration
  encoders:
    num_gin_layers: 3
    gin_hidden_size: 256
    image_encoder_type: "swin_transformer"
    
  # Fusion Configuration
  fusion:
    type: "advanced_modal_fusion"
    cross_attention_layers: 4
    gating_mechanism: true
    
  # Generation Configuration
  generation:
    max_length: 200
    num_beams: 5
    temperature: 0.8
    top_k: 50
    top_p: 0.95
    do_sample: true

# Data Configuration
data:
  # Dataset paths
  train_data: "Datasets/train.csv"
  val_data: "Datasets/validation.csv"
  test_data: "Datasets/test.csv"
  
  # Data processing
  max_text_length: 256
  max_smiles_length: 128
  image_size: [224, 224]
  scaffold_type: "murcko"
  filter_invalid: true
  
  # Input/Output Configuration
  input_modalities: ["text", "smiles"]
  output_modality: "smiles"
  combination_type: 5  # Text + SMILES â†’ SMILES
  
  # Data Augmentation
  augmentation:
    enabled: false
    smiles_enumeration: true
    text_augmentation: true
    augmentation_factor: 2

# Training Configuration
training:
  # Basic Training Parameters
  num_epochs: 100
  batch_size: 32
  eval_batch_size: 64
  gradient_accumulation_steps: 1
  
  # Optimization
  optimizer:
    type: "adamw"
    learning_rate: 5e-5
    weight_decay: 0.01
    eps: 1e-8
    betas: [0.9, 0.999]
    
    # Component-specific learning rates
    lr_multipliers:
      molt5: 0.1
      encoders: 1.0
      fusion: 1.0
      decoders: 1.0
      adapters: 2.0
  
  # Scheduler
  scheduler:
    type: "linear"
    warmup_steps: 1000
    
  # Regularization
  gradient_clip_norm: 1.0
  dropout: 0.1
  
  # Evaluation and Saving
  eval_steps: 1000
  save_steps: 5000
  logging_steps: 100
  eval_epochs: 1
  save_epochs: 10
  
  # Multi-task Learning
  multi_task: false
  task_weights:
    smiles: 1.0
    graph: 0.5
    image: 0.3
  active_tasks: ["smiles"]

# Loss Configuration
loss:
  type: "scaffold"
  generation_weight: 1.0
  scaffold_preservation_weight: 0.5
  validity_weight: 0.3
  preservation_method: "soft"  # "hard" or "soft"
  
  # Multi-modal Loss
  multimodal_loss:
    task_weights:
      smiles: 1.0
      graph: 0.5
      image: 0.3
    alignment_weight: 0.2

# Evaluation Configuration
evaluation:
  # Metrics
  compute_validity: true
  compute_uniqueness: true
  compute_novelty: true
  compute_diversity: true
  compute_scaffold_preservation: true
  compute_drug_likeness: true
  
  # Generation Evaluation
  generation_samples: 10
  similarity_threshold: 0.7
  
  # Benchmark Configuration
  benchmark_tests:
    - "validity_test"
    - "diversity_test"
    - "novelty_test"
    - "scaffold_preservation_test"

# Infrastructure Configuration
infrastructure:
  # Device Configuration
  device: "auto"  # "auto", "cpu", "cuda", or specific GPU
  mixed_precision: true
  
  # Distributed Training
  distributed: false
  world_size: 1
  
  # Data Loading
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  
  # Memory Optimization
  gradient_checkpointing: false
  cpu_offload: false

# Logging Configuration
logging:
  level: "INFO"
  log_to_file: true
  log_file: "logs/training.log"
  
  # Experiment Tracking
  use_tensorboard: true
  use_wandb: false
  wandb_project: "scaffold-mol-generation"
  
  # Visualization
  save_plots: true
  plot_frequency: 1000

# Output Configuration
output:
  # Directories
  output_dir: "outputs"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  results_dir: "results"
  
  # Model Saving
  save_best_model: true
  save_last_model: true
  save_intermediate_checkpoints: true
  
  # Results Saving
  save_predictions: true
  save_evaluation_results: true
  save_visualizations: true

# Reproducibility
reproducibility:
  seed: 42
  deterministic: true
  benchmark: false