# Evaluation configuration for Scaffold-based Molecular Generation

# Inherit from default config
base_config: "default_config.yaml"

# Evaluation-specific settings
evaluation:
  # Model to evaluate
  model_checkpoint: "checkpoints/best_model.pt"
  model_config: "configs/default_config.yaml"
  
  # Evaluation datasets
  datasets:
    test: "Datasets/test.csv"
    validation: "Datasets/validation.csv"
    custom: null  # Path to custom evaluation dataset
  
  # Evaluation parameters
  batch_size: 64
  num_samples: null  # Evaluate all samples (set to number for subset)
  num_workers: 4
  
  # Generation parameters for evaluation
  generation:
    num_samples_per_input: 10  # Generate multiple samples per input
    max_length: 200
    num_beams: 5
    temperature: 0.8
    top_k: 50
    top_p: 0.95
    do_sample: true
    
    # Multiple generation strategies
    strategies:
      - name: "greedy"
        do_sample: false
        num_beams: 1
      
      - name: "beam_search"
        do_sample: false
        num_beams: 5
      
      - name: "sampling"
        do_sample: true
        temperature: 0.8
        top_k: 50
        top_p: 0.95
      
      - name: "diverse_beam"
        do_sample: false
        num_beams: 5
        num_beam_groups: 5
        diversity_penalty: 0.5

  # Comprehensive metrics to compute
  metrics:
    # Basic molecular metrics
    validity: true
    uniqueness: true
    novelty: true
    diversity: true
    
    # Drug-likeness metrics
    lipinski_compliance: true
    qed_score: true
    synthetic_accessibility: false  # Requires additional dependencies
    
    # Scaffold-specific metrics
    scaffold_preservation: true
    scaffold_similarity: true
    scaffold_diversity: true
    
    # Text-molecule alignment (if applicable)
    text_similarity: false
    bleu_score: false
    
    # Advanced metrics
    frechet_chemnet_distance: false  # Requires pre-trained ChemNet
    molecular_fingerprint_similarity: true
    property_prediction: false  # Requires property prediction models
  
  # Benchmark evaluation
  benchmarks:
    # Standard benchmarks
    run_standard_benchmark: true
    
    # Custom benchmarks
    custom_benchmarks:
      - name: "scaffold_preservation_benchmark"
        focus: "scaffold"
        samples: 1000
        metrics: ["scaffold_preservation_rate", "validity", "uniqueness"]
      
      - name: "diversity_benchmark"
        focus: "diversity"
        samples: 2000
        metrics: ["diversity_score", "cluster_count", "mean_pairwise_tanimoto"]
      
      - name: "novelty_benchmark"
        focus: "novelty"
        samples: 1000
        metrics: ["novelty", "valid_count", "novel_count"]

# Statistical analysis
statistical_analysis:
  # Significance testing
  perform_significance_tests: true
  confidence_level: 0.95
  
  # Comparative analysis
  baseline_models: []  # List of baseline model results for comparison
  
  # Bootstrap analysis
  bootstrap_samples: 1000
  bootstrap_confidence_intervals: true

# Visualization and reporting
visualization:
  # Generate visualizations
  create_plots: true
  plot_format: "png"
  plot_dpi: 300
  
  # Visualization types
  plots:
    - "molecular_property_distributions"
    - "scaffold_diversity_analysis"
    - "generation_quality_heatmap"
    - "similarity_distributions"
    - "benchmark_comparison"
  
  # Molecular structure visualization
  draw_molecules: true
  molecules_to_draw: 50  # Number of example molecules to visualize
  molecule_grid_size: [5, 10]  # Grid layout for molecule visualization
  
  # Interactive visualizations (if supported)
  interactive_plots: false

# Output configuration for evaluation
output:
  # Results directory
  results_dir: "evaluation_results"
  
  # File formats
  save_formats: ["json", "csv", "xlsx"]
  
  # Detailed outputs
  save_predictions: true
  save_targets: true
  save_metrics: true
  save_visualizations: true
  save_statistical_analysis: true
  
  # Report generation
  generate_report: true
  report_format: "html"  # "html", "pdf", "markdown"
  include_plots: true

# Ablation studies
ablation_study:
  enabled: false
  
  # Components to ablate
  components:
    - "text_encoder"
    - "fusion_layer"
    - "scaffold_conditioning"
    - "attention_mechanism"
  
  # Ablation parameters
  num_samples: 500  # Smaller sample size for efficiency
  metrics: ["validity", "scaffold_preservation_rate", "uniqueness"]

# Computational efficiency analysis
efficiency_analysis:
  # Measure computational metrics
  measure_inference_time: true
  measure_memory_usage: true
  measure_throughput: true
  
  # Performance benchmarking
  batch_sizes: [1, 8, 16, 32, 64]
  sequence_lengths: [50, 100, 150, 200]
  
  # Hardware profiling
  profile_gpu_usage: true
  profile_cpu_usage: true

# Quality analysis
quality_analysis:
  # Molecular quality filters
  apply_quality_filters: true
  
  quality_filters:
    - name: "lipinski_filter"
      type: "lipinski_rule_of_five"
      strict: false
    
    - name: "pains_filter"
      type: "pains_compounds"
      enabled: false  # Requires PAINS database
    
    - name: "custom_filter"
      type: "molecular_weight"
      min_value: 150
      max_value: 800
  
  # Property distribution analysis
  analyze_property_distributions: true
  reference_datasets: ["chembl", "zinc"]  # For comparison

# Error analysis
error_analysis:
  # Analyze failure cases
  analyze_invalid_molecules: true
  analyze_scaffold_violations: true
  
  # Error categorization
  categorize_errors: true
  error_categories:
    - "invalid_smiles"
    - "scaffold_mismatch"
    - "property_violation"
    - "structural_errors"
  
  # Sample failed cases for manual inspection
  sample_failed_cases: 100

# Custom evaluation protocols
custom_protocols:
  # Domain-specific evaluation
  domain_specific: false
  domains: []  # e.g., ["kinase_inhibitors", "antibiotics"]
  
  # Task-specific evaluation
  task_specific: false
  tasks: []  # e.g., ["lead_optimization", "fragment_elaboration"]
  
  # Property-specific evaluation
  property_specific: false
  target_properties: []  # e.g., ["solubility", "permeability"]