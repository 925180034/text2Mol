#!/usr/bin/env python3
"""
ä¿®å¤çš„å¤šæ¨¡æ€åˆ†å­ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬
è§£å†³tokenizerèŒƒå›´é”™è¯¯å’Œæ— æ•ˆç”Ÿæˆé—®é¢˜
"""

import os
import sys
import argparse
import logging
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from transformers import T5ForConditionalGeneration, T5Tokenizer
from scaffold_mol_gen.models.end2end_model import End2EndMolecularGenerator
from scaffold_mol_gen.training.metrics import GenerationMetrics
from scaffold_mol_gen.utils.mol_utils import MolecularUtils
from scaffold_mol_gen.data.multimodal_preprocessor import MultiModalPreprocessor
from rdkit import Chem

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FixedMultiModalDataset(Dataset):
    """ä¿®å¤çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç¡®ä¿tokenizerå…¼å®¹æ€§"""
    
    def __init__(self, csv_file: str, 
                 molt5_tokenizer: T5Tokenizer,
                 scaffold_modality: str = 'smiles',
                 max_text_length: int = 128,
                 max_smiles_length: int = 128,
                 filter_invalid: bool = True):
        """
        Args:
            csv_file: CSVæ•°æ®æ–‡ä»¶è·¯å¾„
            molt5_tokenizer: MolT5 tokenizer
            scaffold_modality: scaffoldæ¨¡æ€ç±»å‹
            max_text_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
            max_smiles_length: æœ€å¤§SMILESé•¿åº¦
            filter_invalid: æ˜¯å¦è¿‡æ»¤æ— æ•ˆæ•°æ®
        """
        self.tokenizer = molt5_tokenizer
        self.scaffold_modality = scaffold_modality
        self.max_text_length = max_text_length
        self.max_smiles_length = max_smiles_length
        self.preprocessor = MultiModalPreprocessor()
        
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        logger.info(f"åŠ è½½æ•°æ®é›†: {csv_file}")
        df = pd.read_csv(csv_file)
        
        # æ•°æ®æ¸…æ´—å’ŒéªŒè¯
        self.data = []
        valid_count = 0
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†æ•°æ®"):
            scaffold = str(row.get('scaffold', row.get('SMILES', ''))).strip()
            text = str(row.get('description', row.get('text', ''))).strip()
            target_smiles = str(row.get('SMILES', row.get('target', ''))).strip()
            
            # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
            if filter_invalid:
                # éªŒè¯ç›®æ ‡SMILES
                if not target_smiles or target_smiles in ['nan', 'None']:
                    continue
                
                mol = Chem.MolFromSmiles(target_smiles)
                if mol is None:
                    continue
                
                # è§„èŒƒåŒ–SMILES
                target_smiles = Chem.MolToSmiles(mol, canonical=True)
                
                # éªŒè¯scaffoldï¼ˆå¦‚æœæ˜¯SMILESæ¨¡æ€ï¼‰
                if scaffold_modality == 'smiles':
                    if not scaffold or scaffold in ['nan', 'None']:
                        scaffold = ""  # ç©ºscaffold
                    elif Chem.MolFromSmiles(scaffold) is None:
                        continue
                
                # éªŒè¯æ–‡æœ¬æè¿°
                if not text or text in ['nan', 'None']:
                    text = ""  # ç©ºæ–‡æœ¬æè¿°
            
            # æˆªæ–­é•¿åº¦
            text = text[:max_text_length*2]  # é¢„ç•™tokenizationç©ºé—´
            target_smiles = target_smiles[:max_smiles_length*2]
            
            self.data.append({
                'scaffold': scaffold,
                'text': text,
                'target_smiles': target_smiles
            })
            valid_count += 1
        
        logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆ: {len(df)} -> {valid_count} æœ‰æ•ˆæ ·æœ¬")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬ï¼ˆMolT5æ ¼å¼ï¼‰
        scaffold_text = f"Scaffold: {sample['scaffold']}" if sample['scaffold'] else "Scaffold: <empty>"
        description_text = f"Description: {sample['text']}" if sample['text'] else "Description: <empty>"
        input_text = f"{scaffold_text} {description_text}"
        
        # Tokenizeè¾“å…¥
        input_encoding = self.tokenizer(
            input_text,
            max_length=self.max_text_length,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        
        # Tokenizeç›®æ ‡
        target_encoding = self.tokenizer(
            sample['target_smiles'],
            max_length=self.max_smiles_length,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        
        return {
            'input_ids': input_encoding['input_ids'].squeeze(0),
            'attention_mask': input_encoding['attention_mask'].squeeze(0),
            'labels': target_encoding['input_ids'].squeeze(0),
            'target_attention_mask': target_encoding['attention_mask'].squeeze(0),
            'scaffold_data': sample['scaffold'],
            'text_data': sample['text'],
            'target_smiles': sample['target_smiles'],
            'scaffold_modality': self.scaffold_modality
        }


class ConstrainedMultiModalTrainer:
    """çº¦æŸçš„å¤šæ¨¡æ€è®­ç»ƒå™¨ï¼Œé˜²æ­¢ç”Ÿæˆè¶…å‡ºè¯æ±‡è¡¨çš„token"""
    
    def __init__(self,
                 model: End2EndMolecularGenerator,
                 train_dataset: FixedMultiModalDataset,
                 val_dataset: FixedMultiModalDataset,
                 config: Dict[str, Any],
                 device: str = 'cuda'):
        """
        Args:
            model: ç«¯åˆ°ç«¯æ¨¡å‹
            train_dataset: è®­ç»ƒæ•°æ®é›†
            val_dataset: éªŒè¯æ•°æ®é›†
            config: è®­ç»ƒé…ç½®
            device: è®¾å¤‡
        """
        self.model = model.to(device)
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.config = config
        self.device = device
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=config.get('batch_size', 8),
            shuffle=True,
            num_workers=config.get('num_workers', 4),
            drop_last=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=config.get('batch_size', 8),
            shuffle=False,
            num_workers=config.get('num_workers', 4),
            drop_last=False
        )
        
        # è®­ç»ƒå‚æ•°
        self.num_epochs = config.get('num_epochs', 5)
        self.learning_rate = config.get('learning_rate', 1e-4)
        self.weight_decay = config.get('weight_decay', 1e-5)
        self.max_grad_norm = config.get('max_grad_norm', 1.0)
        self.save_interval = config.get('save_interval', 1000)
        self.eval_interval = config.get('eval_interval', 500)
        self.log_interval = config.get('log_interval', 100)
        
        # Tokenizerçº¦æŸ
        self.tokenizer = train_dataset.tokenizer
        self.vocab_size = self.tokenizer.vocab_size
        logger.info(f"Tokenizerè¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        
        # ä¼˜åŒ–å™¨ï¼ˆåªè®­ç»ƒéå†»ç»“å‚æ•°ï¼‰
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        self.optimizer = torch.optim.AdamW(
            trainable_params,
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=self.learning_rate,
            epochs=self.num_epochs,
            steps_per_epoch=len(self.train_loader),
            pct_start=0.1
        )
        
        # è¯„ä»·æŒ‡æ ‡
        self.metrics = GenerationMetrics()
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.epoch = 0
        self.best_val_loss = float('inf')
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path(config.get('output_dir', 'outputs/fixed_training'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(self.output_dir / 'tensorboard')
        
        # ä¿å­˜é…ç½®
        with open(self.output_dir / 'config.json', 'w') as f:
            json.dump(config, f, indent=2)
        
        logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - è®¾å¤‡: {device}")
        logger.info(f"  - è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        logger.info(f"  - éªŒè¯æ ·æœ¬: {len(val_dataset)}")
        logger.info(f"  - æ‰¹å¤§å°: {config.get('batch_size', 8)}")
        logger.info(f"  - å­¦ä¹ ç‡: {self.learning_rate}")
        logger.info(f"  - å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainable_params):,}")
    
    def compute_constrained_loss(self, batch: Dict[str, Any]) -> torch.Tensor:
        """è®¡ç®—çº¦æŸæŸå¤±ï¼Œé˜²æ­¢ç”Ÿæˆæ— æ•ˆtoken"""
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        input_ids = batch['input_ids'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        labels = batch['labels'].to(self.device)
        
        # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹å±æ€§ï¼‰
        outputs = self.model.generator.molt5(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        # åŸºç¡€æŸå¤±
        loss = outputs.loss
        
        # çº¦æŸé¡¹ï¼šé˜²æ­¢ç”Ÿæˆè¶…å‡ºè¯æ±‡è¡¨çš„token
        if hasattr(outputs, 'logits'):
            logits = outputs.logits
            
            # çº¦æŸlogitsåˆ°æœ‰æ•ˆè¯æ±‡è¡¨èŒƒå›´
            # å¯¹è¶…å‡ºè¯æ±‡è¡¨çš„ä½ç½®æ–½åŠ å¤§çš„è´Ÿå€¼
            if logits.size(-1) > self.vocab_size:
                invalid_mask = torch.zeros_like(logits)
                invalid_mask[:, :, self.vocab_size:] = -float('inf')
                logits = logits + invalid_mask
                
                # é‡æ–°è®¡ç®—æŸå¤±
                loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                if labels is not None:
                    # Shift so that tokens < n predict n
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = labels[..., 1:].contiguous()
                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), 
                                  shift_labels.view(-1))
        
        return loss
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(
            self.train_loader,
            desc=f"Epoch {self.epoch + 1}/{self.num_epochs}",
            leave=False
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # è®¡ç®—æŸå¤±
            loss = self.compute_constrained_loss(batch)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(
                    [p for p in self.model.parameters() if p.requires_grad],
                    self.max_grad_norm
                )
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            self.scheduler.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            num_batches += 1
            self.global_step += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{total_loss/num_batches:.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })
            
            # æ—¥å¿—è®°å½•
            if self.global_step % self.log_interval == 0:
                self.writer.add_scalar('train/loss', loss.item(), self.global_step)
                self.writer.add_scalar('train/learning_rate', 
                                     self.optimizer.param_groups[0]['lr'], 
                                     self.global_step)
            
            # éªŒè¯
            if self.global_step % self.eval_interval == 0:
                val_metrics = self.validate()
                self.model.train()  # åˆ‡å›è®­ç»ƒæ¨¡å¼
                
                # è®°å½•éªŒè¯æŒ‡æ ‡
                for key, value in val_metrics.items():
                    self.writer.add_scalar(f'val/{key}', value, self.global_step)
                
                logger.info(f"Step {self.global_step} - Val Loss: {val_metrics.get('loss', 0):.4f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.global_step % self.save_interval == 0:
                self.save_checkpoint(f'checkpoint_step_{self.global_step}.pt')
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        return {'loss': avg_loss}
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        
        total_loss = 0.0
        num_batches = 0
        generated_smiles = []
        target_smiles_list = []
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validating", leave=False):
                # è®¡ç®—æŸå¤±
                loss = self.compute_constrained_loss(batch)
                total_loss += loss.item()
                num_batches += 1
                
                # ç”Ÿæˆæ ·æœ¬ï¼ˆå°‘é‡ç”¨äºè´¨é‡è¯„ä¼°ï¼‰
                if len(generated_smiles) < 50:  # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡
                    try:
                        # ä½¿ç”¨çº¦æŸç”Ÿæˆ
                        input_ids = batch['input_ids'][:2].to(self.device)  # åªå–2ä¸ªæ ·æœ¬
                        attention_mask = batch['attention_mask'][:2].to(self.device)
                        
                        generated_ids = self.model.generator.molt5.generate(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            max_length=64,
                            num_beams=3,
                            temperature=0.9,
                            do_sample=True,
                            pad_token_id=self.tokenizer.pad_token_id,
                            eos_token_id=self.tokenizer.eos_token_id,
                            forced_eos_token_id=self.tokenizer.eos_token_id
                        )
                        
                        # çº¦æŸç”Ÿæˆçš„token IDåˆ°æœ‰æ•ˆèŒƒå›´
                        generated_ids = torch.clamp(generated_ids, 0, self.vocab_size - 1)
                        
                        batch_generated = self.tokenizer.batch_decode(
                            generated_ids,
                            skip_special_tokens=True
                        )
                        
                        batch_targets = batch['target_smiles'][:2]
                        
                        generated_smiles.extend(batch_generated)
                        target_smiles_list.extend(batch_targets)
                        
                    except Exception as e:
                        logger.warning(f"éªŒè¯ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        val_metrics = {'loss': total_loss / num_batches if num_batches > 0 else 0.0}
        
        # è®¡ç®—åˆ†å­è´¨é‡æŒ‡æ ‡
        if generated_smiles and target_smiles_list:
            try:
                # éªŒè¯ç”Ÿæˆçš„SMILES
                valid_generated = []
                valid_targets = []
                for gen, target in zip(generated_smiles, target_smiles_list):
                    if gen and target:
                        # éªŒè¯ç”Ÿæˆçš„SMILES
                        mol = Chem.MolFromSmiles(gen)
                        if mol is not None:
                            valid_generated.append(gen)
                            valid_targets.append(target)
                
                if valid_generated:
                    quality_metrics = self.metrics.compute_metrics(
                        generated_smiles=valid_generated,
                        reference_smiles=valid_targets
                    )
                    val_metrics.update(quality_metrics)
                    
                    logger.info("éªŒè¯æŒ‡æ ‡:")
                    logger.info(f"  ç”Ÿæˆæ ·æœ¬: {len(generated_smiles)}, æœ‰æ•ˆ: {len(valid_generated)}")
                    for key, value in quality_metrics.items():
                        if isinstance(value, (int, float)):
                            logger.info(f"  {key}: {value:.4f}")
                
            except Exception as e:
                logger.warning(f"è´¨é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        
        return val_metrics
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("å¼€å§‹è®­ç»ƒ...")
        logger.info(f"è®­ç»ƒ {self.num_epochs} epochs")
        
        start_time = time.time()
        
        for epoch in range(self.num_epochs):
            self.epoch = epoch
            logger.info(f"\n=== Epoch {epoch + 1}/{self.num_epochs} ===")
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self.train_epoch()
            
            # è®°å½•epochæŒ‡æ ‡
            self.writer.add_scalar('train/epoch_loss', train_metrics['loss'], epoch)
            
            # Epochç»“æŸæ—¶éªŒè¯
            val_metrics = self.validate()
            
            # è®°å½•éªŒè¯æŒ‡æ ‡
            for key, value in val_metrics.items():
                self.writer.add_scalar(f'val_epoch/{key}', value, epoch)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            val_loss = val_metrics.get('loss', float('inf'))
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint('best_model.pt', is_best=True)
                logger.info(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss:.4f})")
            
            # ä¿å­˜epochæ£€æŸ¥ç‚¹
            self.save_checkpoint(f'epoch_{epoch + 1}.pt')
            
            logger.info(f"Epoch {epoch + 1} - Train Loss: {train_metrics['loss']:.4f}, "
                       f"Val Loss: {val_loss:.4f}")
        
        total_time = time.time() - start_time
        logger.info(f"\nâœ… è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/3600:.2f} å°æ—¶")
        
        # æœ€ç»ˆä¿å­˜
        self.save_checkpoint('final_model.pt')
        self.writer.close()
    
    def save_checkpoint(self, filename: str, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config,
            'vocab_size': self.vocab_size
        }
        
        checkpoint_path = self.output_dir / filename
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            # é¢å¤–ä¿å­˜åˆ°easy-to-findä½ç½®
            best_path = self.output_dir / 'model_best.pt'
            torch.save(checkpoint, best_path)


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä¿®å¤çš„å¤šæ¨¡æ€åˆ†å­ç”Ÿæˆæ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--train-data', type=str, default='Datasets/train.csv',
                       help='è®­ç»ƒæ•°æ®CSVæ–‡ä»¶')
    parser.add_argument('--val-data', type=str, default='Datasets/validation.csv',
                       help='éªŒè¯æ•°æ®CSVæ–‡ä»¶')
    parser.add_argument('--output-dir', type=str, 
                       default=f'outputs/fixed_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch-size', type=int, default=4, help='æ‰¹å¤§å°')
    parser.add_argument('--epochs', type=int, default=3, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=5e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--scaffold-modality', type=str, default='smiles',
                       choices=['smiles', 'graph', 'image'], help='Scaffoldæ¨¡æ€')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡')
    parser.add_argument('--sample-size', type=int, default=1000, help='è®­ç»ƒæ ·æœ¬æ•°é™åˆ¶')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(args.train_data):
        logger.error(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.train_data}")
        return
    
    if not os.path.exists(args.val_data):
        logger.error(f"éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.val_data}")
        return
    
    # åˆ›å»ºé…ç½®
    config = {
        'batch_size': args.batch_size,
        'num_epochs': args.epochs,
        'learning_rate': args.lr,
        'weight_decay': 1e-5,
        'max_grad_norm': 1.0,
        'num_workers': 2,
        'save_interval': 500,
        'eval_interval': 200,
        'log_interval': 50,
        'output_dir': args.output_dir,
        'scaffold_modality': args.scaffold_modality,
        'max_text_length': 128,
        'max_smiles_length': 128,
        'filter_invalid': True
    }
    
    logger.info(f"è®­ç»ƒé…ç½®: {json.dumps(config, indent=2)}")
    
    # åˆå§‹åŒ–tokenizer
    molt5_path = "/root/autodl-tmp/text2Mol-models/MolT5-Large-Caption2SMILES"
    tokenizer = T5Tokenizer.from_pretrained(molt5_path)
    
    # åˆ›å»ºæ•°æ®é›†
    try:
        logger.info("åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
        train_dataset = FixedMultiModalDataset(
            csv_file=args.train_data,
            molt5_tokenizer=tokenizer,
            scaffold_modality=config['scaffold_modality'],
            max_text_length=config['max_text_length'],
            max_smiles_length=config['max_smiles_length'],
            filter_invalid=config['filter_invalid']
        )
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if args.sample_size and len(train_dataset.data) > args.sample_size:
            train_dataset.data = train_dataset.data[:args.sample_size]
            logger.info(f"é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°ä¸º: {len(train_dataset.data)}")
        
        logger.info("åˆ›å»ºéªŒè¯æ•°æ®é›†...")
        val_dataset = FixedMultiModalDataset(
            csv_file=args.val_data,
            molt5_tokenizer=tokenizer,
            scaffold_modality=config['scaffold_modality'],
            max_text_length=config['max_text_length'],
            max_smiles_length=config['max_smiles_length'],
            filter_invalid=config['filter_invalid']
        )
        
        # é™åˆ¶éªŒè¯æ ·æœ¬
        if len(val_dataset.data) > 200:
            val_dataset.data = val_dataset.data[:200]
        
        logger.info(f"æ•°æ®é›†åˆ›å»ºå®Œæˆ: train={len(train_dataset)}, val={len(val_dataset)}")
        
    except Exception as e:
        logger.error(f"æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # åˆ›å»ºæ¨¡å‹
    try:
        logger.info("åˆ›å»ºç«¯åˆ°ç«¯æ¨¡å‹...")
        model = End2EndMolecularGenerator(
            hidden_size=768,
            molt5_path=molt5_path,
            use_scibert=False,
            freeze_encoders=True,
            freeze_molt5=False,  # è§£å†»MolT5è¿›è¡Œå¾®è°ƒ
            fusion_type='both',
            device=args.device
        )
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logger.info(f"æ€»å‚æ•°: {total_params:,}, å¯è®­ç»ƒ: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)")
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    try:
        trainer = ConstrainedMultiModalTrainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            config=config,
            device=args.device
        )
        
        # å¼€å§‹è®­ç»ƒ
        logger.info("ğŸš€ å¼€å§‹ä¿®å¤çš„å¤šæ¨¡æ€è®­ç»ƒ...")
        trainer.train()
        logger.info("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        if 'trainer' in locals():
            trainer.save_checkpoint('interrupted_checkpoint.pt')
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()