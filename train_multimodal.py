"""
å¤šæ¨¡æ€åˆ†å­ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬
å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹è®­ç»ƒã€éªŒè¯å’Œä¿å­˜
"""

import os
import sys
import argparse
import logging
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from scaffold_mol_gen.models.end2end_model import End2EndMolecularGenerator
from scaffold_mol_gen.data.multimodal_dataset import create_data_loaders
from scaffold_mol_gen.training.metrics import GenerationMetrics
from scaffold_mol_gen.utils.mol_utils import MolecularUtils

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MultiModalTrainer:
    """å¤šæ¨¡æ€åˆ†å­ç”Ÿæˆæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self,
                 model: End2EndMolecularGenerator,
                 train_loader: DataLoader,
                 val_loader: DataLoader,
                 config: Dict[str, Any],
                 device: str = 'cuda'):
        """
        Args:
            model: ç«¯åˆ°ç«¯æ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            config: è®­ç»ƒé…ç½®
            device: è®¾å¤‡
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = device
        
        # è®­ç»ƒå‚æ•°
        self.num_epochs = config.get('num_epochs', 10)
        self.learning_rate = config.get('learning_rate', 1e-4)
        self.weight_decay = config.get('weight_decay', 1e-5)
        self.warmup_steps = config.get('warmup_steps', 1000)
        self.max_grad_norm = config.get('max_grad_norm', 1.0)
        self.save_interval = config.get('save_interval', 1000)
        self.eval_interval = config.get('eval_interval', 500)
        self.log_interval = config.get('log_interval', 100)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=self.learning_rate,
            epochs=self.num_epochs,
            steps_per_epoch=len(train_loader),
            pct_start=0.1
        )
        
        # è¯„ä»·æŒ‡æ ‡
        self.metrics = GenerationMetrics()
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.epoch = 0
        self.best_val_loss = float('inf')
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path(config.get('output_dir', 'outputs/training'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(self.output_dir / 'tensorboard')
        
        # ä¿å­˜é…ç½®
        with open(self.output_dir / 'config.json', 'w') as f:
            json.dump(config, f, indent=2)
        
        logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - è®¾å¤‡: {device}")
        logger.info(f"  - è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
        logger.info(f"  - éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
        logger.info(f"  - å­¦ä¹ ç‡: {self.learning_rate}")
        logger.info(f"  - è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(
            self.train_loader,
            desc=f"Epoch {self.epoch + 1}/{self.num_epochs}",
            leave=False
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # å‰å‘ä¼ æ’­
            loss = self.train_step(batch)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.max_grad_norm
                )
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            self.scheduler.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            num_batches += 1
            self.global_step += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{total_loss/num_batches:.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })
            
            # æ—¥å¿—è®°å½•
            if self.global_step % self.log_interval == 0:
                self.writer.add_scalar('train/loss', loss.item(), self.global_step)
                self.writer.add_scalar('train/learning_rate', 
                                     self.optimizer.param_groups[0]['lr'], 
                                     self.global_step)
            
            # éªŒè¯
            if self.global_step % self.eval_interval == 0:
                val_metrics = self.validate()
                self.model.train()  # åˆ‡å›è®­ç»ƒæ¨¡å¼
                
                # è®°å½•éªŒè¯æŒ‡æ ‡
                for key, value in val_metrics.items():
                    self.writer.add_scalar(f'val/{key}', value, self.global_step)
                
                logger.info(f"Step {self.global_step} - Val Loss: {val_metrics.get('loss', 0):.4f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.global_step % self.save_interval == 0:
                self.save_checkpoint(f'checkpoint_step_{self.global_step}.pt')
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        return {'loss': avg_loss}
    
    def train_step(self, batch: Dict[str, Any]) -> torch.Tensor:
        """å•æ­¥è®­ç»ƒ"""
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        scaffold_data = batch['scaffold_data']
        text_data = batch['text_data']
        target_smiles = batch['target_smiles']
        scaffold_modality = batch['scaffold_modality']
        
        # è®¡ç®—æŸå¤±
        loss = self.model.compute_loss(
            scaffold_data=scaffold_data,
            text_data=text_data,
            target_smiles=target_smiles,
            scaffold_modality=scaffold_modality
        )
        
        return loss
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        
        total_loss = 0.0
        num_batches = 0
        generated_smiles = []
        target_smiles_list = []
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validating", leave=False):
                # è®¡ç®—æŸå¤±
                loss = self.train_step(batch)
                total_loss += loss.item()
                num_batches += 1
                
                # ç”Ÿæˆæ ·æœ¬ï¼ˆå°‘é‡ç”¨äºè´¨é‡è¯„ä¼°ï¼‰
                if len(generated_smiles) < 100:  # åªè¯„ä¼°å‰100ä¸ªæ ·æœ¬
                    try:
                        scaffold_data = batch['scaffold_data'][:1]  # åªå–ç¬¬ä¸€ä¸ª
                        text_data = batch['text_data'][:1]
                        target = batch['target_smiles'][:1]
                        scaffold_modality = batch['scaffold_modality']
                        
                        generated = self.model.generate(
                            scaffold_data=scaffold_data,
                            text_data=text_data,
                            scaffold_modality=scaffold_modality,
                            num_beams=3,
                            max_length=64
                        )
                        
                        generated_smiles.extend(generated)
                        target_smiles_list.extend(target)
                        
                    except Exception as e:
                        logger.warning(f"ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        val_metrics = {'loss': total_loss / num_batches if num_batches > 0 else 0.0}
        
        # è®¡ç®—åˆ†å­è´¨é‡æŒ‡æ ‡
        if generated_smiles and target_smiles_list:
            try:
                quality_metrics = self.metrics.compute_metrics(
                    generated_smiles=generated_smiles,
                    reference_smiles=target_smiles_list
                )
                val_metrics.update(quality_metrics)
                
                logger.info("éªŒè¯æŒ‡æ ‡:")
                for key, value in quality_metrics.items():
                    if isinstance(value, (int, float)):
                        logger.info(f"  {key}: {value:.4f}")
                
            except Exception as e:
                logger.warning(f"è´¨é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        
        return val_metrics
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("å¼€å§‹è®­ç»ƒ...")
        logger.info(f"è®­ç»ƒ {self.num_epochs} epochs")
        
        start_time = time.time()
        
        for epoch in range(self.num_epochs):
            self.epoch = epoch
            logger.info(f"\n=== Epoch {epoch + 1}/{self.num_epochs} ===")
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self.train_epoch()
            
            # è®°å½•epochæŒ‡æ ‡
            self.writer.add_scalar('train/epoch_loss', train_metrics['loss'], epoch)
            
            # Epochç»“æŸæ—¶éªŒè¯
            val_metrics = self.validate()
            
            # è®°å½•éªŒè¯æŒ‡æ ‡
            for key, value in val_metrics.items():
                self.writer.add_scalar(f'val_epoch/{key}', value, epoch)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            val_loss = val_metrics.get('loss', float('inf'))
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint('best_model.pt', is_best=True)
                logger.info(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss:.4f})")
            
            # ä¿å­˜epochæ£€æŸ¥ç‚¹
            self.save_checkpoint(f'epoch_{epoch + 1}.pt')
            
            logger.info(f"Epoch {epoch + 1} - Train Loss: {train_metrics['loss']:.4f}, "
                       f"Val Loss: {val_loss:.4f}")
        
        total_time = time.time() - start_time
        logger.info(f"\nâœ… è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/3600:.2f} å°æ—¶")
        
        # æœ€ç»ˆä¿å­˜
        self.save_checkpoint('final_model.pt')
        self.writer.close()
    
    def save_checkpoint(self, filename: str, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        checkpoint_path = self.output_dir / filename
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            # é¢å¤–ä¿å­˜åˆ°easy-to-findä½ç½®
            best_path = self.output_dir / 'model_best.pt'
            torch.save(checkpoint, best_path)
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.epoch = checkpoint['epoch']
        self.global_step = checkpoint['global_step']
        self.best_val_loss = checkpoint['best_val_loss']
        
        logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: epoch {self.epoch}, step {self.global_step}")


def create_training_config() -> Dict[str, Any]:
    """åˆ›å»ºé»˜è®¤è®­ç»ƒé…ç½®"""
    return {
        'num_epochs': 5,
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'warmup_steps': 500,
        'max_grad_norm': 1.0,
        'batch_size': 8,
        'num_workers': 4,
        'save_interval': 1000,
        'eval_interval': 500,
        'log_interval': 50,
        'output_dir': f'outputs/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
        'scaffold_modality': 'smiles',
        'max_text_length': 128,
        'max_smiles_length': 128,
        'filter_invalid': True,
        'augment_data': False
    }


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒå¤šæ¨¡æ€åˆ†å­ç”Ÿæˆæ¨¡å‹')
    parser.add_argument('--train-data', type=str, default='Datasets/train.csv',
                       help='è®­ç»ƒæ•°æ®CSVæ–‡ä»¶')
    parser.add_argument('--val-data', type=str, default='Datasets/validation.csv',
                       help='éªŒè¯æ•°æ®CSVæ–‡ä»¶')
    parser.add_argument('--test-data', type=str, default='Datasets/test.csv',
                       help='æµ‹è¯•æ•°æ®CSVæ–‡ä»¶')
    parser.add_argument('--output-dir', type=str, 
                       default=f'outputs/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹å¤§å°')
    parser.add_argument('--epochs', type=int, default=5, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--scaffold-modality', type=str, default='smiles',
                       choices=['smiles', 'graph', 'image'], help='Scaffoldæ¨¡æ€')
    parser.add_argument('--resume', type=str, help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(args.train_data):
        logger.error(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.train_data}")
        return
    
    if not os.path.exists(args.val_data):
        logger.error(f"éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.val_data}")
        return
    
    # åˆ›å»ºé…ç½®
    config = create_training_config()
    config.update({
        'batch_size': args.batch_size,
        'num_epochs': args.epochs,
        'learning_rate': args.lr,
        'output_dir': args.output_dir,
        'scaffold_modality': args.scaffold_modality
    })
    
    logger.info(f"è®­ç»ƒé…ç½®: {json.dumps(config, indent=2)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    try:
        data_loaders = create_data_loaders(
            train_csv=args.train_data,
            val_csv=args.val_data,
            test_csv=args.test_data if os.path.exists(args.test_data) else None,
            batch_size=config['batch_size'],
            num_workers=config['num_workers'],
            scaffold_modality=config['scaffold_modality'],
            max_text_length=config['max_text_length'],
            max_smiles_length=config['max_smiles_length'],
            filter_invalid=config['filter_invalid'],
            augment_data=config['augment_data']
        )
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæ¨¡å‹
    try:
        logger.info("åˆ›å»ºç«¯åˆ°ç«¯æ¨¡å‹...")
        model = End2EndMolecularGenerator(
            hidden_size=768,
            molt5_path="/root/autodl-tmp/text2Mol-models/MolT5-Large-Caption2SMILES",
            use_scibert=False,
            freeze_encoders=True,
            freeze_molt5=True,
            fusion_type='both',
            device=args.device
        )
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiModalTrainer(
        model=model,
        train_loader=data_loaders['train'],
        val_loader=data_loaders['val'],
        config=config,
        device=args.device
    )
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.resume:
        trainer.load_checkpoint(args.resume)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        trainer.train()
        logger.info("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
    except KeyboardInterrupt:
        logger.info("âš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        trainer.save_checkpoint('interrupted_checkpoint.pt')
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()