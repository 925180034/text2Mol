#!/usr/bin/env python3
"""
è®­ç»ƒMolT5-baseæ¨¡å‹ç”ŸæˆSMILES
ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè®©æ¨¡å‹å­¦ä¼šç”Ÿæˆåˆ†å­SMILES
"""

import os
import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import logging
from datetime import datetime
from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW, get_linear_schedule_with_warmup

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleSMILESDataset(Dataset):
    """ç®€å•çš„SMILESç”Ÿæˆæ•°æ®é›†"""
    
    def __init__(self, csv_file, tokenizer, max_length=128):
        self.data = pd.read_csv(csv_file)
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # åªä¿ç•™æœ‰æ•ˆçš„SMILES
        from rdkit import Chem
        valid_mask = self.data['SMILES'].apply(lambda x: Chem.MolFromSmiles(x) is not None)
        self.data = self.data[valid_mask].reset_index(drop=True)
        
        logger.info(f"åŠ è½½äº† {len(self.data)} ä¸ªæœ‰æ•ˆçš„SMILES")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        
        # è¾“å…¥ï¼šæ–‡æœ¬æè¿°
        text = row.get('description', row.get('text', ''))
        if pd.isna(text):
            text = "Generate a molecule"
        
        # æ·»åŠ ä»»åŠ¡å‰ç¼€ï¼ˆå¸®åŠ©æ¨¡å‹ç†è§£ä»»åŠ¡ï¼‰
        input_text = f"Generate SMILES: {text}"
        
        # ç›®æ ‡ï¼šSMILES
        target_smiles = row['SMILES']
        
        # Tokenize
        inputs = self.tokenizer(
            input_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        targets = self.tokenizer(
            target_smiles,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # å°†paddingä½ç½®è®¾ä¸º-100ï¼ˆå¿½ç•¥lossï¼‰
        labels = targets['input_ids'].squeeze()
        labels[labels == self.tokenizer.pad_token_id] = -100
        
        return {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': labels
        }

def train_epoch(model, dataloader, optimizer, scheduler, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    progress_bar = tqdm(dataloader, desc="Training")
    
    for batch in progress_bar:
        # ç§»åŠ¨åˆ°è®¾å¤‡
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        loss = outputs.loss
        total_loss += loss.item()
        
        # åå‘ä¼ æ’­
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({'loss': loss.item()})
    
    return total_loss / len(dataloader)

def evaluate(model, dataloader, tokenizer, device, num_samples=5):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    generated_samples = []
    target_samples = []
    
    with torch.no_grad():
        for i, batch in enumerate(dataloader):
            if i >= num_samples:
                break
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # è®¡ç®—loss
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            total_loss += outputs.loss.item()
            
            # ç”Ÿæˆæ ·æœ¬
            generated_ids = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_length=128,
                num_beams=5,
                do_sample=False,
                early_stopping=True
            )
            
            # è§£ç 
            for gen_id, label in zip(generated_ids, labels):
                gen_text = tokenizer.decode(gen_id, skip_special_tokens=True)
                # æ¸…ç†æ ‡ç­¾ä¸­çš„-100
                label[label == -100] = tokenizer.pad_token_id
                target_text = tokenizer.decode(label, skip_special_tokens=True)
                
                generated_samples.append(gen_text)
                target_samples.append(target_text)
    
    # è®¡ç®—SMILESæœ‰æ•ˆç‡
    from rdkit import Chem
    valid_count = sum(1 for s in generated_samples if Chem.MolFromSmiles(s) is not None)
    validity = valid_count / len(generated_samples) if generated_samples else 0
    
    return total_loss / min(len(dataloader), num_samples), validity, generated_samples, target_samples

def main():
    # è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size = 8
    learning_rate = 5e-5
    num_epochs = 10
    warmup_steps = 500
    
    # è¾“å‡ºç›®å½•
    output_dir = f"/root/autodl-tmp/text2Mol-outputs/molt5_smiles_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    logger.info("="*70)
    logger.info("ğŸš€ è®­ç»ƒMolT5ç”ŸæˆSMILES")
    logger.info("="*70)
    logger.info(f"è®¾å¤‡: {device}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model_path = "/root/autodl-tmp/text2Mol-models/molt5-base"
    logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    model = T5ForConditionalGeneration.from_pretrained(model_path)
    tokenizer = T5Tokenizer.from_pretrained(model_path)
    
    model.to(device)
    
    # å‡†å¤‡æ•°æ®
    logger.info("å‡†å¤‡æ•°æ®é›†...")
    train_dataset = SimpleSMILESDataset(
        "Datasets/train.csv",
        tokenizer
    )
    
    val_dataset = SimpleSMILESDataset(
        "Datasets/validation.csv",
        tokenizer
    )
    
    # é™åˆ¶æ•°æ®é›†å¤§å°ä»¥åŠ å¿«è®­ç»ƒ
    train_dataset.data = train_dataset.data[:5000]
    val_dataset.data = val_dataset.data[:500]
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    logger.info(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    logger.info(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    total_steps = len(train_loader) * num_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    # è®­ç»ƒå¾ªç¯
    best_validity = 0
    
    for epoch in range(num_epochs):
        logger.info(f"\n{'='*50}")
        logger.info(f"Epoch {epoch+1}/{num_epochs}")
        logger.info(f"{'='*50}")
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)
        logger.info(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        
        # è¯„ä¼°
        val_loss, validity, gen_samples, target_samples = evaluate(
            model, val_loader, tokenizer, device
        )
        
        logger.info(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
        logger.info(f"SMILESæœ‰æ•ˆç‡: {validity:.2%}")
        
        # æ‰“å°æ ·æœ¬
        logger.info("\nç”Ÿæˆæ ·æœ¬:")
        for i in range(min(3, len(gen_samples))):
            logger.info(f"  ç›®æ ‡: {target_samples[i]}")
            logger.info(f"  ç”Ÿæˆ: {gen_samples[i]}")
            logger.info("")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if validity > best_validity:
            best_validity = validity
            logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (æœ‰æ•ˆç‡: {validity:.2%})")
            
            # ä¿å­˜æ¨¡å‹
            model.save_pretrained(f"{output_dir}/best_model")
            tokenizer.save_pretrained(f"{output_dir}/best_model")
            
            # ä¿å­˜checkpoint
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_validity': best_validity,
            }, f"{output_dir}/best_checkpoint.pt")
    
    logger.info(f"\n{'='*70}")
    logger.info(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³æœ‰æ•ˆç‡: {best_validity:.2%}")
    logger.info(f"æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
    logger.info(f"{'='*70}")

if __name__ == "__main__":
    main()