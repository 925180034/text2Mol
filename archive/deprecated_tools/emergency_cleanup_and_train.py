#!/usr/bin/env python3
"""
ç´§æ€¥æ¸…ç†å’Œå¤šGPUå¹¶è¡Œè®­ç»ƒç³»ç»Ÿ
"""

import os
import sys
import shutil
import subprocess
import time
import threading
import signal
from pathlib import Path
import datetime
import psutil
import torch

class MultiModalTrainingManager:
    def __init__(self):
        self.base_output_dir = "/root/autodl-tmp/text2Mol-outputs/safe_training"
        self.log_dir = Path("logs")
        self.log_dir.mkdir(exist_ok=True)
        
        # è®­ç»ƒé…ç½®
        self.modalities = ['smiles', 'graph', 'image']
        self.gpu_assignments = {
            'smiles': 0,
            'graph': 1, 
            'image': 0  # SMILESè®­ç»ƒå®Œæˆåä½¿ç”¨GPU 0
        }
        
        # æ¿€è¿›çš„æ¸…ç†é…ç½®
        self.MAX_CHECKPOINTS_PER_MODALITY = 2  # æ¯ä¸ªæ¨¡æ€æœ€å¤šä¿ç•™2ä¸ªcheckpoint
        self.CHECK_INTERVAL = 30  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
        self.DISK_THRESHOLD = 85  # ç£ç›˜ä½¿ç”¨è¶…è¿‡85%å°±æ¸…ç†
        self.MIN_FREE_GB = 10  # ä¿æŒè‡³å°‘10GBå¯ç”¨ç©ºé—´
        
        self.monitoring = True
        self.training_processes = {}
        
    def get_disk_info(self):
        """è·å–ç£ç›˜ä½¿ç”¨ä¿¡æ¯"""
        disk_usage = shutil.disk_usage("/root/autodl-tmp")
        return {
            'total_gb': disk_usage.total / (1024**3),
            'used_gb': disk_usage.used / (1024**3),
            'free_gb': disk_usage.free / (1024**3),
            'used_percent': (disk_usage.used / disk_usage.total) * 100
        }
    
    def emergency_cleanup(self):
        """ç´§æ€¥æ¸…ç†æ‰€æœ‰å¤šä½™çš„checkpoint"""
        print("\nğŸš¨ æ‰§è¡Œç´§æ€¥ç£ç›˜æ¸…ç†...")
        total_freed = 0
        
        for modality in self.modalities:
            modality_dir = Path(self.base_output_dir) / modality
            if not modality_dir.exists():
                continue
                
            print(f"\næ¸…ç† {modality} æ¨¡æ€:")
            
            # è·å–æ‰€æœ‰checkpointæ–‡ä»¶
            checkpoints = []
            for file in modality_dir.glob("*.pt"):
                stat = file.stat()
                checkpoints.append({
                    'path': file,
                    'name': file.name,
                    'size': stat.st_size,
                    'mtime': stat.st_mtime
                })
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
            checkpoints.sort(key=lambda x: x['mtime'], reverse=True)
            
            # ä¿ç•™ç­–ç•¥
            keep_files = set()
            
            # 1. ä¿ç•™æœ€æ–°çš„checkpoint
            if checkpoints:
                keep_files.add(checkpoints[0]['path'])
            
            # 2. ä¿ç•™best_model
            for ckpt in checkpoints:
                if 'best' in ckpt['name'].lower():
                    keep_files.add(ckpt['path'])
                    break
            
            # 3. å¦‚æœç©ºé—´å…è®¸ï¼Œå†ä¿ç•™ä¸€ä¸ª
            if len(checkpoints) > 2 and len(keep_files) < self.MAX_CHECKPOINTS_PER_MODALITY:
                for ckpt in checkpoints[1:]:
                    if ckpt['path'] not in keep_files:
                        keep_files.add(ckpt['path'])
                        break
            
            # åˆ é™¤å…¶ä»–æ–‡ä»¶
            for ckpt in checkpoints:
                if ckpt['path'] not in keep_files:
                    try:
                        size_gb = ckpt['size'] / (1024**3)
                        ckpt['path'].unlink()
                        total_freed += size_gb
                        print(f"  âŒ åˆ é™¤: {ckpt['name']} ({size_gb:.1f}GB)")
                    except Exception as e:
                        print(f"  âš ï¸ åˆ é™¤å¤±è´¥: {ckpt['name']} - {e}")
                else:
                    size_gb = ckpt['size'] / (1024**3)
                    print(f"  âœ… ä¿ç•™: {ckpt['name']} ({size_gb:.1f}GB)")
        
        print(f"\nâœ… æ¸…ç†å®Œæˆï¼Œé‡Šæ”¾äº† {total_freed:.1f}GB")
        
        # æ˜¾ç¤ºæ¸…ç†åçŠ¶æ€
        disk_info = self.get_disk_info()
        print(f"æ¸…ç†å: {disk_info['used_percent']:.1f}% ä½¿ç”¨, {disk_info['free_gb']:.1f}GB å¯ç”¨")
        
        return total_freed
    
    def continuous_monitoring(self):
        """æŒç»­ç›‘æ§ç£ç›˜å¹¶è‡ªåŠ¨æ¸…ç†"""
        print("\nğŸ” å¯åŠ¨æŒç»­ç£ç›˜ç›‘æ§...")
        
        while self.monitoring:
            try:
                disk_info = self.get_disk_info()
                
                # æ¯30ç§’æ˜¾ç¤ºçŠ¶æ€
                print(f"\n[{datetime.datetime.now().strftime('%H:%M:%S')}] ç£ç›˜ç›‘æ§:")
                print(f"  ä½¿ç”¨: {disk_info['used_percent']:.1f}% | å¯ç”¨: {disk_info['free_gb']:.1f}GB")
                
                # æ£€æŸ¥æ´»è·ƒçš„è®­ç»ƒè¿›ç¨‹
                active_count = sum(1 for pid in self.training_processes.values() 
                                 if pid and psutil.pid_exists(pid))
                print(f"  æ´»è·ƒè®­ç»ƒ: {active_count}/{len(self.training_processes)}")
                
                # è§¦å‘æ¸…ç†çš„æ¡ä»¶
                if (disk_info['used_percent'] > self.DISK_THRESHOLD or 
                    disk_info['free_gb'] < self.MIN_FREE_GB):
                    print(f"  âš ï¸ è§¦å‘è‡ªåŠ¨æ¸…ç† (ä½¿ç”¨ç‡>{self.DISK_THRESHOLD}% æˆ– å¯ç”¨<{self.MIN_FREE_GB}GB)")
                    self.emergency_cleanup()
                
                time.sleep(self.CHECK_INTERVAL)
                
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
                time.sleep(10)
    
    def create_optimized_config(self, modality, gpu_id):
        """åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
        config_path = Path(f"configs/multi_gpu_{modality}_config.yaml")
        config_path.parent.mkdir(exist_ok=True)
        
        # é’ˆå¯¹åŒGPUä¼˜åŒ–çš„é…ç½®
        configs = {
            'smiles': {
                'batch_size': 24,  # å¢å¤§batch size
                'gradient_accumulation': 1,
                'learning_rate': 3e-5,
                'save_frequency': 3000,  # å‡å°‘ä¿å­˜é¢‘ç‡
            },
            'graph': {
                'batch_size': 16,
                'gradient_accumulation': 2,
                'learning_rate': 2e-5,
                'save_frequency': 2500,
            },
            'image': {
                'batch_size': 8,
                'gradient_accumulation': 4,
                'learning_rate': 1e-5,
                'save_frequency': 2000,
            }
        }
        
        config = configs[modality]
        
        config_content = f"""# å¤šGPUè®­ç»ƒé…ç½® - {modality}æ¨¡æ€ (GPU {gpu_id})
# ç”Ÿæˆæ—¶é—´: {datetime.datetime.now()}

data:
  train_file: 'Datasets/train.csv'
  val_file: 'Datasets/validation.csv'
  test_file: 'Datasets/test.csv'
  modality: '{modality}'
  batch_size: {config['batch_size']}
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2

model:
  hidden_size: 768
  num_layers: 6
  num_heads: 12
  dropout: 0.1
  fusion_type: 'both'
  use_cached_molt5: true

training:
  num_epochs: 5
  learning_rate: {config['learning_rate']}
  warmup_steps: 500
  gradient_accumulation_steps: {config['gradient_accumulation']}
  max_grad_norm: 1.0
  
  # ä¼˜åŒ–çš„checkpointç­–ç•¥
  save_frequency: {config['save_frequency']}
  save_total_limit: 2  # åªä¿ç•™2ä¸ªcheckpoint
  save_best_only: false
  save_on_epoch_end: true
  
  # ç£ç›˜ä¿æŠ¤
  disk_space_threshold: 5.0
  auto_cleanup: true
  cleanup_keep_count: 2

optimization:
  fp16: true  # æ··åˆç²¾åº¦è®­ç»ƒ
  gradient_checkpointing: true
  find_unused_parameters: false
  dataloader_drop_last: true
  
device:
  gpu_id: {gpu_id}
  
logging:
  log_every_n_steps: 100
  tensorboard: true
  wandb: false
"""
        
        config_path.write_text(config_content)
        return str(config_path)
    
    def start_modality_training(self, modality, gpu_id):
        """å¯åŠ¨å•ä¸ªæ¨¡æ€çš„è®­ç»ƒ"""
        print(f"\nğŸš€ åœ¨GPU {gpu_id} ä¸Šå¯åŠ¨ {modality} æ¨¡æ€è®­ç»ƒ")
        
        # åˆ›å»ºé…ç½®
        config_path = self.create_optimized_config(modality, gpu_id)
        
        # å‡†å¤‡è¾“å‡ºç›®å½•
        output_dir = Path(self.base_output_dir) / modality
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ—¥å¿—æ–‡ä»¶
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.log_dir / f"multi_gpu_{modality}_{timestamp}.log"
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            sys.executable,
            "train_multimodal.py",
            "--config", config_path,
            "--output_dir", str(output_dir),
            "--modality", modality,
        ]
        
        # è®¾ç½®ç¯å¢ƒå˜é‡æŒ‡å®šGPU
        env = os.environ.copy()
        env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"ä½¿ç”¨GPU: {gpu_id}")
        print(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
        
        # å¯åŠ¨è¿›ç¨‹
        with open(log_file, 'w') as f:
            process = subprocess.Popen(
                cmd,
                stdout=f,
                stderr=subprocess.STDOUT,
                env=env,
                preexec_fn=os.setsid
            )
        
        self.training_processes[modality] = process.pid
        print(f"âœ… {modality} è®­ç»ƒå·²å¯åŠ¨ (PID: {process.pid})")
        
        return process.pid
    
    def start_all_training(self):
        """å¯åŠ¨æ‰€æœ‰æ¨¡æ€çš„è®­ç»ƒ"""
        print("\nğŸ¯ å¯åŠ¨å¤šæ¨¡æ€å¹¶è¡Œè®­ç»ƒç³»ç»Ÿ")
        print("=" * 60)
        
        # é¦–å…ˆæ¸…ç†ç£ç›˜
        disk_info = self.get_disk_info()
        print(f"åˆå§‹ç£ç›˜çŠ¶æ€: {disk_info['used_percent']:.1f}% ä½¿ç”¨, {disk_info['free_gb']:.1f}GB å¯ç”¨")
        
        if disk_info['free_gb'] < 15:
            print("âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œæ‰§è¡Œæ¸…ç†...")
            self.emergency_cleanup()
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=self.continuous_monitoring)
        monitor_thread.daemon = True
        monitor_thread.start()
        print("âœ… ç£ç›˜ç›‘æ§å·²å¯åŠ¨")
        
        # æ£€æŸ¥GPU
        gpu_count = torch.cuda.device_count()
        print(f"\næ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        for i in range(gpu_count):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        
        # å¯åŠ¨è®­ç»ƒ
        print("\nå¼€å§‹å¯åŠ¨è®­ç»ƒä»»åŠ¡:")
        
        # 1. åœ¨GPU 0ä¸Šå¯åŠ¨SMILES
        self.start_modality_training('smiles', 0)
        time.sleep(5)  # ç­‰å¾…è¿›ç¨‹å¯åŠ¨
        
        # 2. åœ¨GPU 1ä¸Šå¯åŠ¨Graph  
        self.start_modality_training('graph', 1)
        time.sleep(5)
        
        # 3. åœ¨GPU 0ä¸Šå¯åŠ¨Imageï¼ˆä¸SMILESå…±äº«ï¼‰
        self.start_modality_training('image', 0)
        
        print("\nâœ… æ‰€æœ‰è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨!")
        print("\nç›‘æ§å‘½ä»¤:")
        print("  æŸ¥çœ‹GPUä½¿ç”¨: nvidia-smi -l 1")
        print("  æŸ¥çœ‹æ—¥å¿—: tail -f logs/multi_gpu_*.log")
        print("  åœæ­¢æ‰€æœ‰è®­ç»ƒ: python emergency_cleanup_and_train.py --stop")
        
    def stop_all_training(self):
        """åœæ­¢æ‰€æœ‰è®­ç»ƒ"""
        print("\nğŸ›‘ åœæ­¢æ‰€æœ‰è®­ç»ƒè¿›ç¨‹...")
        
        for modality, pid in self.training_processes.items():
            if pid and psutil.pid_exists(pid):
                try:
                    os.killpg(os.getpgid(pid), signal.SIGTERM)
                    print(f"  âœ… åœæ­¢ {modality} (PID: {pid})")
                except:
                    print(f"  âš ï¸ æ— æ³•åœæ­¢ {modality} (PID: {pid})")
        
        self.monitoring = False
        print("âœ… æ‰€æœ‰è®­ç»ƒå·²åœæ­¢")

def main():
    if len(sys.argv) > 1 and sys.argv[1] == '--stop':
        # åœæ­¢æ¨¡å¼
        manager = MultiModalTrainingManager()
        manager.stop_all_training()
    else:
        # å¯åŠ¨æ¨¡å¼
        manager = MultiModalTrainingManager()
        
        # é¦–å…ˆæ‰§è¡Œç´§æ€¥æ¸…ç†
        print("ğŸ§¹ é¦–å…ˆæ‰§è¡Œç´§æ€¥æ¸…ç†...")
        manager.emergency_cleanup()
        
        # ç„¶åå¯åŠ¨è®­ç»ƒ
        manager.start_all_training()
        
        # ä¿æŒä¸»è¿›ç¨‹è¿è¡Œ
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("\nâš ï¸ æ”¶åˆ°ä¸­æ–­ä¿¡å·")
            manager.stop_all_training()

if __name__ == "__main__":
    main()